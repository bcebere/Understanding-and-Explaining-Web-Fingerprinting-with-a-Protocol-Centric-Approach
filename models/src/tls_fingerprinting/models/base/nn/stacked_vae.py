# stdlib
from typing import Any, List, Optional, Tuple

# third party
import numpy as np
import torch
from pydantic import validate_arguments
from torch import Tensor, nn
from torch.optim import Adam
from torch.utils.data import DataLoader, TensorDataset, sampler
from tqdm import tqdm

# tls_fingerprinting absolute
import tls_fingerprinting.logger as log
from tls_fingerprinting.utils.constants import DEVICE

# tls_fingerprinting relative
from .vae import VAE


class StackedVAE(nn.Module):
    """
    .. inheritance-diagram:: tls_fingerprinting.plugins.core.models.base.nn.stacked_vae.StackedVAE
        :parts: 1


    StackedVAE implementation.

    Args:
        n_features: int
            Number of features in the dataset
        n_units_embedding: int
            Number of units in the latent space
        batch_size: int
            Training batch size
        n_iter: int
            Number of training iterations
        random_state: int
            Random random_state
        lr: float
            Learning rate
        weight_decay: float:
            Optimizer weight decay
        decoder_n_layers_hidden: int
            Number of hidden layers in the decoder
        decoder_n_units_hidden: int
            Number of units in the hidden layer in the decoder
        decoder_nonlin_out: List
            List of activations layout, as generated by the tabular encoder
        decoder_batch_norm: bool
            Use batchnorm in the decoder
        decoder_dropout: float
            Use dropout in the decoder
        decoder_residual: bool
            Use residual connections in the decoder
        encoder_n_layers_hidden: int
            Number of hidden layers in the encoder
        encoder_n_units_hidden: int
            Number of units in the hidden layer in the encoder
        encoder_batch_norm: bool
            Use batchnorm in the encoder
        encoder_dropout: float
            Use dropout in the encoder
        encoder_residual: bool
            Use residual connections in the encoder
        loss_strategy: str
            - standard: classic VAE loss
        loss_factor: int
            Parameter for the standard loss
        dataloader_sampler:
            Custom sampler used by the dataloader, useful for conditional sampling.
        device:
            CPU/CUDA
        clipping_value:
            Gradients clipping value. Zero disables the feature
        # early stopping
        n_iter_print: int
            Number of iterations after which to print updates and check the validation loss.
        n_iter_min: int
            Minimum number of iterations to go through before starting early stopping
        patience: int
            Max number of iterations without any improvement before early stopping is trigged.
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        n_features: int,
        n_units_embedding: int,
        n_units_conditional: int = 0,
        n_stacks: int = 3,
        batch_size: int = 100,
        n_iter: int = 500,
        random_state: int = 0,
        lr: float = 2e-4,
        weight_decay: float = 1e-3,
        # Decoder
        decoder_n_layers_hidden: int = 1,
        decoder_n_units_hidden: int = 200,
        decoder_nonlin: str = "leaky_relu",
        decoder_nonlin_out: Optional[List[Tuple[str, int]]] = None,
        decoder_batch_norm: bool = False,
        decoder_dropout: float = 0,
        decoder_residual: bool = False,
        # Encoder
        encoder_n_layers_hidden: int = 1,
        encoder_n_units_hidden: int = 200,
        encoder_nonlin: str = "leaky_relu",
        encoder_batch_norm: bool = False,
        encoder_dropout: float = 0,
        # Loss parameters
        loss_factor: int = 2,
        dataloader_sampler: Optional[sampler.Sampler] = None,
        device: Any = DEVICE,
        clipping_value: int = 1,
        # early stopping
        n_iter_min: int = 100,
        n_iter_print: int = 10,
        patience: int = 5,
    ) -> None:
        super(StackedVAE, self).__init__()

        loss_strategy = "standard"
        torch.manual_seed(random_state)

        self.vaes = [
            VAE(
                n_features=n_features,
                n_units_embedding=n_units_embedding,
                n_units_conditional=n_units_conditional,
                random_state=random_state,
                # Decoder
                decoder_n_layers_hidden=decoder_n_layers_hidden,
                decoder_n_units_hidden=decoder_n_units_hidden,
                decoder_nonlin=decoder_nonlin,
                decoder_batch_norm=decoder_batch_norm,
                decoder_dropout=decoder_dropout,
                decoder_residual=decoder_residual,
                # Encoder
                encoder_n_layers_hidden=encoder_n_layers_hidden,
                encoder_n_units_hidden=encoder_n_units_hidden,
                encoder_nonlin=encoder_nonlin,
                encoder_batch_norm=encoder_batch_norm,
                encoder_dropout=encoder_dropout,
                device=device,
            ),
        ]
        for n_stack in range(n_stacks - 1):
            self.vaes.append(
                VAE(
                    n_features=(2**n_stack) * n_units_embedding,
                    n_units_embedding=(2 ** (n_stack + 1)) * n_units_embedding,
                    n_units_conditional=n_units_conditional,
                    random_state=random_state,
                    # Decoder
                    decoder_n_layers_hidden=decoder_n_layers_hidden,
                    decoder_n_units_hidden=decoder_n_units_hidden,
                    decoder_nonlin=decoder_nonlin,
                    decoder_batch_norm=decoder_batch_norm,
                    decoder_dropout=decoder_dropout,
                    decoder_residual=decoder_residual,
                    # Encoder
                    encoder_n_layers_hidden=encoder_n_layers_hidden,
                    encoder_n_units_hidden=encoder_n_units_hidden,
                    encoder_nonlin=encoder_nonlin,
                    encoder_batch_norm=encoder_batch_norm,
                    encoder_dropout=encoder_dropout,
                    device=device,
                ),
            )

        self.device = device
        self.batch_size = batch_size
        self.n_iter = n_iter
        self.loss_factor = loss_factor
        self.lr = lr
        self.weight_decay = weight_decay
        self.loss_strategy = loss_strategy
        self.dataloader_sampler = dataloader_sampler
        self.random_state = random_state
        self.clipping_value = clipping_value
        self.n_iter_print = n_iter_print
        self.n_iter_min = n_iter_min
        self.patience = patience
        self.n_units_conditional = n_units_conditional
        self.n_units_embedding = (2 ** (n_stacks - 1)) * n_units_embedding

        if decoder_nonlin_out is None:
            decoder_nonlin_out = [("none", n_features)]
        self.decoder_nonlin_out = decoder_nonlin_out

    def to(self, device: Any) -> Any:
        self.device = device
        for idx, vae in enumerate(self.vaes):
            self.vaes[idx].to(device)

        return self

    def fit(
        self,
        X: np.ndarray,
        cond: Optional[np.ndarray] = None,
    ) -> Any:
        Xt = self._check_tensor(X)
        condt: Optional[torch.Tensor] = None

        if self.n_units_conditional > 0:
            if cond is None:
                raise ValueError("Expecting valid conditional for training")
            if len(cond.shape) == 1:
                cond = cond.reshape(-1, 1)
            if cond.shape[1] != self.n_units_conditional:
                raise ValueError(
                    "Expecting conditional with n_units = {self.n_units_conditional}"
                )
            if cond.shape[0] != X.shape[0]:
                raise ValueError(
                    "Expecting conditional with the same length as the dataset"
                )

            condt = self._check_tensor(cond)

        self._train(Xt, condt)

        return self

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def generate(self, count: int, cond: Optional[np.ndarray] = None) -> np.ndarray:
        self.eval()

        steps = count // self.batch_size + 1
        data = []

        condt: Optional[torch.Tensor] = None
        if cond is None and self.n_units_conditional > 0:
            # sample from the original conditional
            if self._original_cond is None:
                raise ValueError("Invalid original conditional. Provide a valid value.")
            cond_idxs = torch.randint(len(self._original_cond), (count,))
            cond = self._original_cond[cond_idxs]

        if cond is not None and len(cond.shape) == 1:
            cond = cond.reshape(-1, 1)

        if cond is not None and len(cond) != count:
            raise ValueError("cond length must match count")

        if cond is not None:
            condt = self._check_tensor(cond)

        for idx in range(steps):
            mean = torch.zeros(self.batch_size, self.n_units_embedding)
            std = torch.ones(self.batch_size, self.n_units_embedding)
            noise = torch.normal(mean=mean, std=std).to(self.device)

            condt_mb: Optional[torch.Tensor] = None
            if condt is not None:
                condt_mb = condt[
                    idx * self.batch_size : min((idx + 1) * self.batch_size, count)
                ]
                noise = noise[: len(condt_mb)]

            fake = self.decode(noise, condt_mb)
            data.append(fake.detach().cpu().numpy())

        data = np.concatenate(data, axis=0)
        data = data[:count]
        return data

    def _train_test_split(self, X: torch.Tensor, cond: Optional[torch.Tensor]) -> Tuple:
        if self.dataloader_sampler is not None:
            train_idx, test_idx = self.dataloader_sampler.train_test()
        else:
            total = np.arange(0, len(X))
            np.random.shuffle(total)
            split = int(len(total) * 0.8)
            train_idx, test_idx = total[:split], total[split:]

        X_train, X_val = X[train_idx], X[test_idx]
        cond_train, cond_test = None, None
        if cond is not None:
            cond_train, cond_test = cond[train_idx], cond[test_idx]

        return X_train, X_val, cond_train, cond_test

    def encode(
        self, X: torch.Tensor, cond: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        embedding = X
        for vae in self.vaes:
            embedding, mu, logvar = vae.encode(embedding, cond)

        return embedding, mu, logvar

    def decode(
        self, noise: torch.Tensor, cond: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        reconstructed = noise
        for vae in reversed(self.vaes):
            reconstructed = vae.decode(reconstructed, cond)

        return reconstructed

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _train(
        self,
        X: Tensor,
        cond: Optional[torch.Tensor] = None,
    ) -> Any:
        self._original_cond = cond

        X, X_val, cond, cond_val = self._train_test_split(X, cond)
        loader = self._dataloader(X, cond)

        params = []
        for vae in self.vaes:
            params.extend(list(vae.parameters()))
        optimizer = Adam(
            params,
            weight_decay=self.weight_decay,
            lr=self.lr,
        )

        best_loss = np.inf
        best_state_dict = None
        patience = 0
        for epoch in tqdm(range(self.n_iter)):
            self.train()
            for id_, data in enumerate(loader):
                cond_mb: Optional[torch.Tensor] = None

                if self.n_units_conditional > 0:
                    X, cond_mb = data
                else:
                    X = data[0]

                embedding, mu, logvar = self.encode(X, cond_mb)
                reconstructed = self.decode(embedding, cond_mb)

                loss = self._loss_function(
                    reconstructed,
                    X,
                    mu,
                    logvar,
                    cond_mb,
                )
                optimizer.zero_grad()
                if self.clipping_value > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.parameters(), self.clipping_value
                    )
                loss.backward()
                optimizer.step()

            if epoch % self.n_iter_print == 0:
                self.eval()
                embedding, mu, logvar = self.encode(X_val, cond_val)
                reconstructed = self.decode(embedding, cond_val)

                val_loss = (
                    self._loss_function(
                        reconstructed,
                        X_val,
                        mu,
                        logvar,
                        cond_val,
                    )
                    .detach()
                    .item()
                )

                log.debug(f"[{epoch}/{self.n_iter}] Loss: {val_loss}")
                if val_loss >= best_loss:
                    patience += 1
                else:
                    best_loss = val_loss
                    best_state_dict = self.state_dict()
                    patience = 0

                if patience >= self.patience and epoch >= self.n_iter_min:
                    log.debug(f"[{epoch}/{self.n_iter}] Early stopping")
                    break

        if best_state_dict is not None:
            self.load_state_dict(best_state_dict)

        return self

    def _check_tensor(self, X: Tensor) -> Tensor:
        if isinstance(X, Tensor):
            return X.to(self.device)
        else:
            return torch.from_numpy(np.asarray(X)).to(self.device)

    def _dataloader(self, X: Tensor, cond: Optional[torch.Tensor] = None) -> DataLoader:
        if cond is None:
            dataset = TensorDataset(X)
        else:
            dataset = TensorDataset(X, cond)

        return DataLoader(
            dataset,
            sampler=self.dataloader_sampler,
            batch_size=self.batch_size,
            pin_memory=False,
        )

    def _loss_function(
        self,
        reconstructed: Tensor,
        real: Tensor,
        mu: Tensor,
        logvar: Tensor,
        cond: Optional[torch.Tensor] = None,
    ) -> Tensor:
        loss = self._loss_function_standard(reconstructed, real, mu, logvar)

        return loss

    def _loss_function_standard(
        self,
        reconstructed: Tensor,
        real: Tensor,
        mu: Tensor,
        logvar: Tensor,
    ) -> Tensor:
        step = 0

        loss = []
        for activation, length in self.decoder_nonlin_out:
            step_end = step + length
            # reconstructed is after the activation
            if activation == "softmax":
                discr_loss = nn.NLLLoss(reduction="sum")(
                    torch.log(reconstructed[:, step:step_end] + 1e-8),
                    torch.argmax(real[:, step:step_end], dim=-1),
                )
                loss.append(discr_loss)
            else:
                diff = reconstructed[:, step:step_end] - real[:, step:step_end]
                cont_loss = (50 * diff**2).sum()

                loss.append(cont_loss)
            step = step_end

        if step != reconstructed.size()[1]:
            raise RuntimeError(
                f"Invalid reconstructed features. Expected {step}, got {reconstructed.shape}"
            )

        reconstruction_loss = torch.sum(torch.stack(loss)) / real.shape[0]
        KLD_loss = (-0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())) / real.shape[
            0
        ]

        if torch.isnan(reconstruction_loss):
            raise RuntimeError("NaNs detected in the reconstruction_loss")
        if torch.isnan(KLD_loss):
            raise RuntimeError("NaNs detected in the KLD_loss")

        return reconstruction_loss * self.loss_factor + KLD_loss
