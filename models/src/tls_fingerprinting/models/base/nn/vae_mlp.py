# stdlib
from typing import Any, List, Optional, Tuple

# third party
import numpy as np
import torch
from pydantic import validate_arguments
from torch import Tensor, nn
from torch.utils.data import sampler

# tls_fingerprinting absolute
from tls_fingerprinting.utils.constants import DEVICE

# tls_fingerprinting relative
# tls_crawler relative
from .mlp import MLP
from .vae import VAE


class VAEMLP(nn.Module):
    """
    .. inheritance-diagram:: tls_fingerprinting.plugins.core.models.base.nn.vae_mlp.VAE_MLP
        :parts: 1


    Basic VAE implementation.

    Args:
        n_features: int
            Number of features in the dataset
        n_units_embedding: int
            Number of units in the latent space
        batch_size: int
            Training batch size
        n_iter: int
            Number of training iterations
        random_state: int
            Random random_state
        lr: float
            Learning rate
        weight_decay: float:
            Optimizer weight decay
        decoder_n_layers_hidden: int
            Number of hidden layers in the decoder
        decoder_n_units_hidden: int
            Number of units in the hidden layer in the decoder
        decoder_nonlin_out: List
            List of activations layout, as generated by the tabular encoder
        decoder_batch_norm: bool
            Use batchnorm in the decoder
        decoder_dropout: float
            Use dropout in the decoder
        decoder_residual: bool
            Use residual connections in the decoder
        output_nonlin_out: List
            List of activations layout, as generated by the tabular encoder
        output_batch_norm: bool
            Use batchnorm in the output
        output_dropout: float
            Use dropout in the output
        output_residual: bool
            Use residual connections in the output layer
        encoder_n_layers_hidden: int
            Number of hidden layers in the encoder
        encoder_n_units_hidden: int
            Number of units in the hidden layer in the encoder
        encoder_batch_norm: bool
            Use batchnorm in the encoder
        encoder_dropout: float
            Use dropout in the encoder
        encoder_residual: bool
            Use residual connections in the encoder
        loss_strategy: str
            - standard: classic VAE loss
        loss_factor: int
            Parameter for the standard loss
        dataloader_sampler:
            Custom sampler used by the dataloader, useful for conditional sampling.
        device:
            CPU/CUDA
        clipping_value:
            Gradients clipping value. Zero disables the feature
        # early stopping
        n_iter_print: int
            Number of iterations after which to print updates and check the validation loss.
        n_iter_min: int
            Minimum number of iterations to go through before starting early stopping
        patience: int
            Max number of iterations without any improvement before early stopping is trigged.
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        task_type: str,  # classification, regression
        n_features: int,
        n_units_embedding: int,
        n_units_out: int,
        batch_size: int = 100,
        n_iter: int = 500,
        random_state: int = 0,
        lr: float = 2e-4,
        weight_decay: float = 1e-3,
        # Decoder
        decoder_n_layers_hidden: int = 1,
        decoder_n_units_hidden: int = 200,
        decoder_nonlin: str = "leaky_relu",
        decoder_nonlin_out: Optional[List[Tuple[str, int]]] = None,
        decoder_batch_norm: bool = False,
        decoder_dropout: float = 0,
        decoder_residual: bool = False,
        # Encoder
        encoder_n_layers_hidden: int = 1,
        encoder_n_units_hidden: int = 200,
        encoder_nonlin: str = "leaky_relu",
        encoder_batch_norm: bool = False,
        encoder_dropout: float = 0,
        # Output
        output_n_layers_hidden: int = 1,
        output_n_units_hidden: int = 200,
        output_nonlin: str = "leaky_relu",
        output_batch_norm: bool = False,
        output_dropout: float = 0.1,
        output_residual: bool = False,
        # Loss parameters
        loss_factor: int = 2,
        dataloader_sampler: Optional[sampler.Sampler] = None,
        device: Any = DEVICE,
        clipping_value: int = 1,
        # early stopping
        n_iter_min: int = 100,
        n_iter_print: int = 10,
        patience: int = 5,
    ) -> None:
        super(VAEMLP, self).__init__()

        torch.manual_seed(random_state)
        self.task_type = task_type
        self.device = device

        self.vae = VAE(
            n_features=n_features,
            n_units_embedding=n_units_embedding,
            batch_size=batch_size,
            n_iter=n_iter,
            random_state=random_state,
            lr=lr,
            weight_decay=weight_decay,
            # Decoder
            decoder_n_layers_hidden=decoder_n_layers_hidden,
            decoder_n_units_hidden=decoder_n_units_hidden,
            decoder_nonlin=decoder_nonlin,
            decoder_nonlin_out=decoder_nonlin_out,
            decoder_batch_norm=decoder_batch_norm,
            decoder_dropout=decoder_dropout,
            decoder_residual=decoder_residual,
            # Encoder
            encoder_n_layers_hidden=encoder_n_layers_hidden,
            encoder_n_units_hidden=encoder_n_units_hidden,
            encoder_nonlin=encoder_nonlin,
            encoder_batch_norm=encoder_batch_norm,
            encoder_dropout=encoder_dropout,
            # Loss parameters
            loss_factor=loss_factor,
            dataloader_sampler=dataloader_sampler,
            device=device,
            clipping_value=clipping_value,
            # early stopping
            n_iter_min=n_iter_min,
            n_iter_print=n_iter_print,
            patience=patience,
        )
        self.prediction = MLP(
            task_type=task_type,
            n_units_in=n_units_embedding,
            n_units_out=n_units_out,
            n_units_hidden=output_n_units_hidden,
            n_layers_hidden=output_n_layers_hidden,
            nonlin=output_nonlin,
            dropout=output_dropout,
            batch_norm=output_batch_norm,
            residual=output_residual,
            # training
            device=device,
            batch_size=batch_size,
            n_iter=n_iter,
            random_state=random_state,
            lr=lr,
            weight_decay=weight_decay,
        )

    def to(self, device: Any) -> Any:
        self.device = device
        self.vae.to(device)
        self.prediction.to(device)

        return self

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        cond: Optional[np.ndarray] = None,
    ) -> Any:
        Xt = self._check_tensor(X)
        yt = self._check_tensor(y)

        self.vae.fit(Xt)
        encoded, _, _ = self.vae.encode(Xt)
        encoded = encoded.detach()

        self.prediction.fit(encoded, yt)

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        Xt = self._check_tensor(X)

        encoded, _, _ = self.vae.encode(Xt)
        encoded = encoded.detach().cpu().numpy()

        return self.prediction.predict(encoded)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        Xt = self._check_tensor(X)

        encoded, _, _ = self.vae.encode(Xt)
        encoded = encoded.detach().cpu().numpy()

        return self.prediction.predict_proba(encoded)

    def _check_tensor(self, X: Tensor) -> Tensor:
        if isinstance(X, Tensor):
            return X.to(self.device)
        else:
            return torch.from_numpy(np.asarray(X)).to(self.device)
